{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN_cnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LujNBJMmOjUP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"455e627d-4715-41ec-b539-6b04ac1b4618","executionInfo":{"status":"ok","timestamp":1566040713924,"user_tz":-540,"elapsed":567,"user":{"displayName":"한상준","photoUrl":"","userId":"11527713768863174571"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F0tfBWc3Rr3Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0818b326-1f84-4a3c-f50c-d5bab14867e0","executionInfo":{"status":"ok","timestamp":1566040716959,"user_tz":-540,"elapsed":531,"user":{"displayName":"한상준","photoUrl":"","userId":"11527713768863174571"}}},"source":["import os\n","os.chdir('/content/drive/My Drive/cifar10_project')\n","os.getcwd()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/cifar10_project'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"ej-MTHKRRr1z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":442},"outputId":"ef57a8fb-5085-43be-8689-2c6a251551f7","executionInfo":{"status":"ok","timestamp":1566040722498,"user_tz":-540,"elapsed":2888,"user":{"displayName":"한상준","photoUrl":"","userId":"11527713768863174571"}}},"source":["from tensorflow.examples.tutorials.mnist import input_data\n","\n","mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0817 11:18:40.654163 139739535169408 deprecation.py:323] From <ipython-input-3-46c62fe1e279>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","W0817 11:18:40.655506 139739535169408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","W0817 11:18:40.658789 139739535169408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./mnist/data/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["W0817 11:18:40.930082 139739535169408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","W0817 11:18:40.935473 139739535169408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.one_hot on tensors.\n","W0817 11:18:40.986994 139739535169408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n","Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n","Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0o2XCamJRrzR","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","def conv2d(inputs,filters, kernel_size = 3, strides = 1, activation = None, batch_norm = None):\n","    x = tf.layers.conv2d(inputs, filters = filters, kernel_size=kernel_size, strides=strides, padding = 'SAME',activation=None)\n","    if batch_norm:   \n","        x = tf.layers.batch_normalization(x)\n","    x = activation(x)\n","    return x\n","    \n","def conv2d_transpose(inputs, filters, kernel_size = 2, strides = 2, activation = None, batch_norm = None):\n","    x = tf.layers.conv2d_transpose(inputs, filters = filters, kernel_size = kernel_size, strides = strides, padding='SAME', activation = None)\n","    if batch_norm:      \n","        x = tf.layers.batch_normalization(x)\n","    x = activation(x)\n","    return x\n","  \n","  \n","class GAN_cnn():\n","    def __init__(self, input_shape = [28,28,1], n_noise=128):\n","    \n","        self.input_shape = input_shape\n","        self.n_noise = n_noise\n","        \n","        self.h, self.w = self.input_shape[0]//4, self.input_shape[1]//4\n","        assert self.h*4 == self.input_shape[0] and self.w*4 ==self.input_shape[1], 'input_shape이 적절하지 않습니다.'\n","                \n","    def get_noise(self, batch_size):\n","        return np.random.uniform(-1.,1., size = [batch_size, self.n_noise])\n","\n","    def generator(self, noise, labels):\n","        with tf.variable_scope('generator',reuse = tf.AUTO_REUSE):\n","            \n","            inputs = tf.concat([noise,labels], 1)\n","\n","            x = tf.layers.dense(inputs, self.h*self.w*256, activation = tf.nn.relu)\n","            x = tf.reshape(x, shape = [-1, self.h, self.w, 256])\n","                     \n","            x = conv2d_transpose(x, filters = 128, kernel_size = 2, strides = 2, activation = tf.nn.relu)\n","            x = conv2d_transpose(x, filters = 64, kernel_size = 2, strides = 2, activation = tf.nn.relu)\n","            x = conv2d(x, filters = self.input_shape[-1], kernel_size = 3, strides = 1, activation = tf.nn.sigmoid)\n","            return x\n","        \n","    def discriminator(self, inputs, labels):\n","        with tf.variable_scope('discriminator',reuse = tf.AUTO_REUSE):\n","\n","            x = tf.layers.flatten(inputs)\n","            x = tf.concat([x, labels], 1)\n","            x = tf.layers.dense(x, 256, activation = tf.nn.relu)\n","            x = tf.layers.dense(x, 1, activation = None)\n","            \n","            return x\n"," "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"28gvUtBZXffc","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","tf.reset_default_graph()\n","\n","input_shape = [28,28,1]\n","n_class = 10\n","n_noise=128\n","\n","model = GAN_cnn(input_shape = input_shape, n_noise = n_noise)\n","\n","\n","X = tf.placeholder(tf.float32, [None]+input_shape)\n","Y = tf.placeholder(tf.float32, [None,n_class])\n","Z = tf.placeholder(tf.float32, [None,n_noise])\n","\n","G = model.generator(Z,Y)\n","D_real = model.discriminator(X, Y) \n","D_gene = model.discriminator(G,Y)\n","  \n","loss_D_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","                        logits=D_real, labels=tf.ones_like(D_real)))\n","loss_D_gene = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","                        logits=D_gene, labels=tf.zeros_like(D_gene)))\n","loss_D = loss_D_real + loss_D_gene\n","loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n","                        logits=D_gene, labels = tf.ones_like(D_gene)))\n","        \n","vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n","                                  scope = 'discriminator')\n","vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n","                                  scope = 'generator')\n","train_D = tf.train.AdamOptimizer().minimize(loss_D,\n","                                             var_list = vars_D)\n","train_G = tf.train.AdamOptimizer().minimize(loss_G,\n","                                             var_list = vars_G)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6r4nVBlIRrxD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":914},"outputId":"a012d84e-1272-4a88-e29e-dbceeb0f6d89","executionInfo":{"status":"ok","timestamp":1566041587903,"user_tz":-540,"elapsed":345884,"user":{"displayName":"한상준","photoUrl":"","userId":"11527713768863174571"}}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","total_epoch = 50\n","batch_size = 100\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","total_batch = int(mnist.train.num_examples/batch_size)\n","loss_val_D, loss_val_G = 0, 0\n","\n","for epoch in range(total_epoch):\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        batch_xs = np.reshape(batch_xs,[-1,28,28,1])\n","        noise = model.get_noise(batch_size)\n","\n","        _, cost1 = sess.run([train_D, loss_D],\n","                                 feed_dict={X: batch_xs, Y: batch_ys, Z: noise})\n","        _, cost2 = sess.run([train_G, loss_G],\n","                                 feed_dict={Y: batch_ys, Z: noise})\n","        loss_val_D += cost1/total_batch\n","        loss_val_G += cost2/total_batch\n","   \n","    print('Epoch:', '%04d' % epoch,\n","          'D loss: {:.4}'.format(loss_val_D),\n","          'G loss: {:.4}'.format(loss_val_G))\n","\n","        \n","    if epoch == 0 or (epoch + 1) % 10 == 0:\n","        sample_size = 10\n","        noise = model.get_noise(sample_size)\n","        samples = sess.run(G,\n","                           feed_dict={Y: mnist.test.labels[:sample_size],\n","                                      Z: noise})\n","\n","        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n","\n","        for i in range(sample_size):\n","            ax[0][i].set_axis_off()\n","            ax[1][i].set_axis_off()\n","\n","            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n","            ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n","\n","        plt.savefig('samples/cnn_mnist{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n","        plt.close(fig)\n","\n","print('done')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Epoch: 0000 D loss: 0.6278 G loss: 2.934\n","Epoch: 0001 D loss: 1.268 G loss: 6.338\n","Epoch: 0002 D loss: 1.877 G loss: 8.862\n","Epoch: 0003 D loss: 2.414 G loss: 11.5\n","Epoch: 0004 D loss: 2.907 G loss: 14.24\n","Epoch: 0005 D loss: 3.372 G loss: 17.19\n","Epoch: 0006 D loss: 3.769 G loss: 20.35\n","Epoch: 0007 D loss: 4.103 G loss: 23.74\n","Epoch: 0008 D loss: 4.398 G loss: 27.39\n","Epoch: 0009 D loss: 4.696 G loss: 31.14\n","Epoch: 0010 D loss: 5.058 G loss: 35.02\n","Epoch: 0011 D loss: 5.769 G loss: 38.23\n","Epoch: 0012 D loss: 6.532 G loss: 40.65\n","Epoch: 0013 D loss: 7.381 G loss: 42.72\n","Epoch: 0014 D loss: 8.286 G loss: 44.59\n","Epoch: 0015 D loss: 9.254 G loss: 46.18\n","Epoch: 0016 D loss: 10.29 G loss: 47.61\n","Epoch: 0017 D loss: 11.37 G loss: 49.15\n","Epoch: 0018 D loss: 12.45 G loss: 50.5\n","Epoch: 0019 D loss: 13.56 G loss: 51.73\n","Epoch: 0020 D loss: 14.69 G loss: 52.96\n","Epoch: 0021 D loss: 15.84 G loss: 54.14\n","Epoch: 0022 D loss: 16.99 G loss: 55.43\n","Epoch: 0023 D loss: 18.13 G loss: 56.62\n","Epoch: 0024 D loss: 19.3 G loss: 57.71\n","Epoch: 0025 D loss: 20.48 G loss: 58.8\n","Epoch: 0026 D loss: 21.66 G loss: 59.89\n","Epoch: 0027 D loss: 22.83 G loss: 60.97\n","Epoch: 0028 D loss: 24.01 G loss: 62.05\n","Epoch: 0029 D loss: 25.2 G loss: 63.12\n","Epoch: 0030 D loss: 26.39 G loss: 64.17\n","Epoch: 0031 D loss: 27.57 G loss: 65.24\n","Epoch: 0032 D loss: 28.76 G loss: 66.29\n","Epoch: 0033 D loss: 29.95 G loss: 67.33\n","Epoch: 0034 D loss: 31.13 G loss: 68.37\n","Epoch: 0035 D loss: 32.32 G loss: 69.42\n","Epoch: 0036 D loss: 33.5 G loss: 70.49\n","Epoch: 0037 D loss: 34.69 G loss: 71.51\n","Epoch: 0038 D loss: 35.88 G loss: 72.57\n","Epoch: 0039 D loss: 37.06 G loss: 73.63\n","Epoch: 0040 D loss: 38.24 G loss: 74.7\n","Epoch: 0041 D loss: 39.41 G loss: 75.76\n","Epoch: 0042 D loss: 40.58 G loss: 76.82\n","Epoch: 0043 D loss: 41.75 G loss: 77.88\n","Epoch: 0044 D loss: 42.92 G loss: 78.94\n","Epoch: 0045 D loss: 44.08 G loss: 80.03\n","Epoch: 0046 D loss: 45.24 G loss: 81.12\n","Epoch: 0047 D loss: 46.39 G loss: 82.2\n","Epoch: 0048 D loss: 47.54 G loss: 83.31\n","Epoch: 0049 D loss: 48.68 G loss: 84.42\n","done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NffVHT9XTRmk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}